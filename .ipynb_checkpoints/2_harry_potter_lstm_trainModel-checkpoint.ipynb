{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------\n",
    "# from github Dodo\n",
    "# 2019-7-7\n",
    "# harry_potter_lstm.py 训练模型\n",
    "#   generate_text.py: 文本生成\n",
    "#   训练结果：（给定首字母\"Hi, \"）\n",
    "#   Hi, he was nearly off at Harry to say the time that and she had been back to his staircase of the too the Hermione?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "def LoadData(filename):\n",
    "    # param:\n",
    "    #      filename - 读取的文件名字\n",
    "    # return:\n",
    "    #      vocab:     文本包含的所有字符集合\n",
    "    #      vocab2Int: 字符到编码的映射\n",
    "    #      int2Vocab: 编码到字符的集合\n",
    "    #      encode:    编码后的文本\n",
    "    text = open(filename,encoding = 'utf-8').read()\n",
    "    \n",
    "    # 放进集合，集合乱序并且是只出现一次\n",
    "    vocab      = set(text)\n",
    "    vocabList  = list(vocab)\n",
    "    vocabList.sort()\n",
    "    \n",
    "    # 词 ->index\n",
    "    vocab2Int = {word:index for index,word in enumerate(vocabList)}\n",
    "    \n",
    "    #index->词\n",
    "    int2Vocab = {index:word for word,index in vocab2Int.items()}\n",
    "    \n",
    "    #encoding for the file\n",
    "    encode = np.array([vocab2Int[word] for word in text])\n",
    "    return vocab,vocab2Int,int2Vocab,encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_patch(input_data,n_seqs,n_sequence_length):\n",
    "    # para:\n",
    "    #   input_data/np.array : 等待拆分的数据\n",
    "    #   n_seqs     : 一个batch中多少个样本/一个batch中含有几句话/句子的数目\n",
    "    #   n_sequence_length: 一段话的长度/一段话中有多少个字符/一个样本中有多少字符\n",
    "    \n",
    "    # 一个batch的size 是句子数目* 每个句子的长度\n",
    "    batch_size  = n_seqs*n_sequence_length\n",
    "    \n",
    "    # input_data中能分词多少个batch\n",
    "    batch_num = len(input_data)//batch_size # //取到整数\n",
    "    \n",
    "    # 获取整数个batch_size的input_data,不够一个batchsize的丢掉\n",
    "    input_data = input_data[: batch_num*batch_size]\n",
    "    \n",
    "    # 将数据inputdata变成n_seqs行的二维数组,列数表示的是样本长度\n",
    "    # 取一个batch的数据相当于行n_seqs 不变，在列的索引位置上量出n_sequence_length长度切一刀\n",
    "    input_data = input_data.reshape(n_seqs,-1) # 列数不限制\n",
    "    \n",
    "    # 获取batch data\n",
    "    for i in range(0,input_data.shape[1],n_sequence_length):\n",
    "        x = input_data[:,i:i+n_sequence_length]\n",
    "        y = np.zero_like(x)\n",
    "        \n",
    "        # LSTM的前一个输出y是后一个输入x,所有x和y实际上是错开一位的\n",
    "        #下面将x向前移动一位转成y\n",
    "        y[:,: -1] = x[:,1:]  # 将x第2列及以后的元素放到y第一列开始的位置\n",
    "        y[:,-1]   = x[:,0]   # 将x的第1列放到y的最后1列的位置\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_input(n_seqs,n_sequence_length):\n",
    "    # para:\n",
    "    #    n_seqs: 每个输入的样本输入\n",
    "    #    n_sequence_length: 每个样本的长度\n",
    "    # return:\n",
    "    #\n",
    "    # 初始的两个占位\n",
    "    #  输入的样本大小是n_seqs*n_sequence_length\n",
    "    #  因为每个字符会对应灭个字符的输出，所以target 和input 大小一致\n",
    "    inputs = tf.placeholder(dtype = tf.int32,shape = (n_seqs,n_sequence_length), name = 'inputs')\n",
    "    target = tf.placeholder(dtype = tf.int32,shape = (n_seqs,n_sequence_length),name = 'target')\n",
    "    \n",
    "    return inputs,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_lstm(lstm_unit_nums,keep_prob,nums_layers,n_seqs):\n",
    "    # param:\n",
    "    # lstm_unit_nums: 每个Lstm节点中的隐藏节点的个数\n",
    "    # keep_prob :     drop 的比例\n",
    "    # nums_layers:    lstm的层数目\n",
    "    # n_seqs:         每次传入的样本的个数\n",
    "    # \n",
    "    # 在老版本的tensorflow上搭建LStm的过程: 1.先新建一个lstm节点，然后给节点添加drop，然后把节点转换成列表传入\n",
    "    # MultiRNNCell上\n",
    "    # 在当前的tensorflow版本上，需要这样\n",
    "    #    创建一个列表，如果创建3层lstm,那么要初始化3个lstm节点，添加drop并放入列表中，在通过MultiRNNcell 生成\n",
    "    #    \n",
    "    # 创建列表，后续生成的节点都放到这个节点里\n",
    "    lstm = []\n",
    "    # 循环创建层\n",
    "    for i in range(nums_layers):\n",
    "        # 单独创建一层的lstm节点\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = lstm_unit_nums)\n",
    "        \n",
    "        #添加drop\n",
    "        drop = tf.nn.run_cell.DropoutWrapper(cell = cell,output_keep_prob = keep_prob)\n",
    "        lstm.append(drop)\n",
    "    \n",
    "    # 搭建多lstm\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(lstm)\n",
    "    #初始化输入状态\n",
    "    init_state = cell.zero_state(batch_size = n_seqs, dtype=tf.float32)\n",
    "    return cell,init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_output\n",
    "def model_output(lstm_out,in_size,out_size):\n",
    "    '''\n",
    "    模型输出: lstm的输出后再通过softmax运算\n",
    "    模型输入矩阵的大小是n_seqs * n_sequence_length,lstm的隐层节点是lstm_unit_nums,所有lstm的输出大小是:\n",
    "    [n_seqs,n_sequence_length,lstm_unit_nums].这里需要转成二维: [n_seqs*n_sequence_length,lstm_unit_nums]\n",
    "    \n",
    "    中间权重w层应该是 [lstm_unit_nums,len(vocab)]\n",
    "    \n",
    "    softmax层输出的大小是:len(vocab)\n",
    "    \n",
    "    params:\n",
    "        lstm_out: lstm的输出,在这里len(vocab)\n",
    "        in_size:  lstm输出的大小，在这里是lstm_unit_nums\n",
    "        out_size: len(vocab) 词汇表大小\n",
    "    '''\n",
    "    # 先将lstm的三维的output转成2维\n",
    "    lstm_out = tf.reshape(lstm_out,shape = (-1,in_size))  # in_size = lstm_unit_nums \n",
    "    \n",
    "    # tensorflow 提供variable_scope 来共享变量\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal(shape= (in_size,out_size)),\n",
    "                                stddev=0.1,dtype=tf.float32,name= 'softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(shape = (out_size)), dtype = tf.float32,name = 'softmax_b')\n",
    "        \n",
    "    # 计算输出\n",
    "    logits = tf.matmul(lstm_out,softmax_w) + softmax_b\n",
    "    # 计算输出的softmax\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    return output,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mode loss 计算交叉熵\n",
    "def model_loss(target,logits,num_class):\n",
    "    # 计算交叉熵\n",
    "    #  target：标签\n",
    "    #  logits:预测输出\n",
    "    #  num_class: 字符的种类数目\n",
    "    #\n",
    "    # 将target 生成one_hot编码\n",
    "    y_one_hot = tf.one_hot(target,num_class)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(label = y_one_hot,logits = logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_optimizer(learning_rate,loss,clip_val):\n",
    "    # param\n",
    "    # learning_rate:学习率\n",
    "    # loss :损失函数\n",
    "    # clip_val: 为了避免梯度爆炸而使用裁剪梯度\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
